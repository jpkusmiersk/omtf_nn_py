{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e108fe-72a6-4053-bec2-af1a368594f2",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d419c1d5-26ec-42d6-a6fb-e076dd903666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, time\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e9384f-3b1f-4210-b8cf-9cda27c00a8a",
   "metadata": {},
   "source": [
    "## Networks definitions and adaptations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef53a8d-4d7c-4d1d-b914-8429d43fa8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classic NN definitions:\n",
      "dense_layer1_size: 128\n",
      "dense_layer2_size: 64\n",
      "dense_layer3_size: 32\n",
      "dense_layer4_size: 1\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "from architecture_definitions import *\n",
    "\n",
    "dir_postfix = get_classic_nn_dir_postfix() \n",
    "    \n",
    "print_Classic_NN()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630c86e0-bf1f-4a56-abf8-5762ea739a8f",
   "metadata": {},
   "source": [
    "### Training data set preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "165627dd-9c1c-41ea-b97a-ff97949c379a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from files:\n",
      "/scratch_cmsse/alibordi/data/training/SingleMu_OneOverPt_April4_chunk_0_filtered.tfrecord.gzip\n",
      "/scratch_cmsse/alibordi/data/training/SingleMu_OneOverPt_Feb15_chunk_0_filtered.tfrecord.gzip\n",
      "/scratch_cmsse/alibordi/data/training/SingleMu_OneOverPt_Feb22_chunk_0_filtered.tfrecord.gzip\n",
      "/scratch_cmsse/alibordi/data/training/SingleMu_OneOverPt_April14_chunk_0_filtered.tfrecord.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 07:20:10.804431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-20 07:20:10.814471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-20 07:20:10.814784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-20 07:20:10.816579: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-20 07:20:10.818375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-20 07:20:10.818670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-20 07:20:10.818918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-20 07:20:11.237217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-20 07:20:11.237344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-20 07:20:11.237438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-20 07:20:11.237518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6658 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset <ParallelMapDataset element_spec=(TensorSpec(shape=(4096, 37), dtype=tf.float16, name=None), (TensorSpec(shape=(4096,), dtype=tf.float16, name=None),), TensorSpec(shape=(4096,), dtype=tf.float16, name=None))>\n"
     ]
    }
   ],
   "source": [
    "import io_functions as io\n",
    "importlib.reload(io)\n",
    "\n",
    "batchSize = 4096\n",
    "nEpochs = 1\n",
    "\n",
    "#trainDataDir = \"/scratch_ssd/akalinow/ProgrammingProjects/MachineLearning/OMTF/data/18_12_2020/\"   \n",
    "trainDataDir = \"/home/kbunkow/cms_data/OMTF_data_2020/18_12_2020/\"\n",
    "trainFileNames = glob.glob(trainDataDir+'OMTFHits_pats0x0003_oldSample_files_*_chunk_0.tfrecord.gzip')\n",
    "\n",
    "trainDataDir = \"/scratch_cmsse/alibordi/data/training/\"\n",
    "trainFileNames = glob.glob(trainDataDir+'*OneOverPt*tfrecord.gzip')\n",
    "\n",
    "dataset = io.get_Classic_NN_dataset(batchSize, nEpochs, trainFileNames, isTrain=True)\n",
    "\n",
    "print(\"dataset\", dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44abffd6-e9d9-47f0-9198-a84e8c7ac6b6",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d414507a-ce56-42f5-8edf-0a4064d77c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " pt_layer_1 (Dense)          (None, 128)               4864      \n",
      "                                                                 \n",
      " pt_layer_2 (Dense)          (None, 64)                8256      \n",
      "                                                                 \n",
      " pt_layer_3 (Dense)          (None, 32)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,233\n",
      "Trainable params: 15,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import model_functions as models\n",
    "importlib.reload(models)\n",
    "\n",
    "import io_functions as io\n",
    "importlib.reload(io)\n",
    "\n",
    "networkInputSize = 2 * np.sum(io.getFeaturesMask()) + 1\n",
    "loss_fn = 'mape'\n",
    "loss_fn = loss_MAPE_MAE\n",
    "\n",
    "model = models.get_Classic_NN(networkInputSize=networkInputSize, loss_fn=loss_fn)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42a01e5-c7aa-4f68-87f1-c6a4e6270723",
   "metadata": {},
   "source": [
    "### The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c510c79-fee3-4799-8a21-f8365f8107a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 2070 SUPER, compute capability 7.5\n",
      "Compute dtype: float16\n",
      "Variable dtype: float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 07:20:12.052659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "from keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a48d01be-95ac-416b-8577-c23d3f8924b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start. Current Time = 2023_Apr_20_07_20_12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 07:20:12.066990: I tensorflow/core/profiler/lib/profiler_session.cc:99] Profiler session initializing.\n",
      "2023-04-20 07:20:12.067012: I tensorflow/core/profiler/lib/profiler_session.cc:114] Profiler session started.\n",
      "2023-04-20 07:20:12.067045: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1665] Profiler found 1 GPUs\n",
      "2023-04-20 07:20:12.220357: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session tear down.\n",
      "2023-04-20 07:20:12.221599: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1799] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "     10/Unknown - 1s 21ms/step - loss: 2203.9333"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 07:20:13.280801: I tensorflow/core/profiler/lib/profiler_session.cc:99] Profiler session initializing.\n",
      "2023-04-20 07:20:13.280829: I tensorflow/core/profiler/lib/profiler_session.cc:114] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18/Unknown - 1s 18ms/step - loss: 1350.9750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 07:20:13.626779: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2023-04-20 07:20:13.638479: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1799] CUPTI activity buffer flushed\n",
      "2023-04-20 07:20:13.964183: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:521]  GpuTracer has collected 1671 callback api events and 1650 activity events. \n",
      "2023-04-20 07:20:14.236445: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session tear down.\n",
      "2023-04-20 07:20:14.882529: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/fit/2023_Apr_20_07_20_12_classic_128_64_32_1/plugins/profile/2023_04_20_07_20_14\n",
      "\n",
      "2023-04-20 07:20:15.660221: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to logs/fit/2023_Apr_20_07_20_12_classic_128_64_32_1/plugins/profile/2023_04_20_07_20_14/fba800be2533.trace.json.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     30/Unknown - 4s 98ms/step - loss: 916.1741 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 07:20:15.920378: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/fit/2023_Apr_20_07_20_12_classic_128_64_32_1/plugins/profile/2023_04_20_07_20_14\n",
      "\n",
      "2023-04-20 07:20:15.925525: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/2023_Apr_20_07_20_12_classic_128_64_32_1/plugins/profile/2023_04_20_07_20_14/fba800be2533.memory_profile.json.gz\n",
      "2023-04-20 07:20:15.931707: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/fit/2023_Apr_20_07_20_12_classic_128_64_32_1/plugins/profile/2023_04_20_07_20_14\n",
      "Dumped tool data for xplane.pb to logs/fit/2023_Apr_20_07_20_12_classic_128_64_32_1/plugins/profile/2023_04_20_07_20_14/fba800be2533.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/fit/2023_Apr_20_07_20_12_classic_128_64_32_1/plugins/profile/2023_04_20_07_20_14/fba800be2533.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/fit/2023_Apr_20_07_20_12_classic_128_64_32_1/plugins/profile/2023_04_20_07_20_14/fba800be2533.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/fit/2023_Apr_20_07_20_12_classic_128_64_32_1/plugins/profile/2023_04_20_07_20_14/fba800be2533.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/fit/2023_Apr_20_07_20_12_classic_128_64_32_1/plugins/profile/2023_04_20_07_20_14/fba800be2533.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787/787 [==============================] - 22s 27ms/step - loss: 164.5494 - val_loss: 90.8507\n",
      "Epoch 2/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 97.4770 - val_loss: 97.9156\n",
      "Epoch 3/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 89.8830 - val_loss: 80.0709\n",
      "Epoch 4/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 85.2027 - val_loss: 82.9249\n",
      "Epoch 5/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 81.6556 - val_loss: 77.5365\n",
      "Epoch 6/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 78.7384 - val_loss: 77.3029\n",
      "Epoch 7/150\n",
      "360/787 [============>.................] - ETA: 9s - loss: 77.3541\n",
      "Epoch 7: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0007.ckpt\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 76.4882 - val_loss: 73.6245\n",
      "Epoch 8/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 74.4335 - val_loss: 73.8082\n",
      "Epoch 9/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 72.8091 - val_loss: 74.5328\n",
      "Epoch 10/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 71.4371 - val_loss: 71.8001\n",
      "Epoch 11/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 70.2751 - val_loss: 70.9024\n",
      "Epoch 12/150\n",
      "787/787 [==============================] - 18s 23ms/step - loss: 69.3418 - val_loss: 72.2813\n",
      "Epoch 13/150\n",
      "725/787 [==========================>...] - ETA: 1s - loss: 68.5187\n",
      "Epoch 13: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0013.ckpt\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 68.5049 - val_loss: 69.3737\n",
      "Epoch 14/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 67.8138 - val_loss: 69.9033\n",
      "Epoch 15/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 67.1671 - val_loss: 69.5853\n",
      "Epoch 16/150\n",
      "787/787 [==============================] - 18s 23ms/step - loss: 66.6757 - val_loss: 68.7727\n",
      "Epoch 17/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 66.2108 - val_loss: 67.8050\n",
      "Epoch 18/150\n",
      "787/787 [==============================] - 18s 23ms/step - loss: 65.7972 - val_loss: 67.5591\n",
      "Epoch 19/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 65.4531 - val_loss: 65.9047\n",
      "Epoch 20/150\n",
      "301/787 [==========>...................] - ETA: 10s - loss: 65.5903\n",
      "Epoch 20: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0020.ckpt\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 65.1696 - val_loss: 66.7311\n",
      "Epoch 21/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 64.8997 - val_loss: 65.4751\n",
      "Epoch 22/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 64.6769 - val_loss: 66.3585\n",
      "Epoch 23/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 64.4832 - val_loss: 65.4501\n",
      "Epoch 24/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 64.3191 - val_loss: 65.0259\n",
      "Epoch 25/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 64.1568 - val_loss: 64.9583\n",
      "Epoch 26/150\n",
      "662/787 [========================>.....] - ETA: 2s - loss: 64.0488\n",
      "Epoch 26: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0026.ckpt\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 64.0151 - val_loss: 64.8706\n",
      "Epoch 27/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.9050 - val_loss: 65.0180\n",
      "Epoch 28/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.8118 - val_loss: 64.8172\n",
      "Epoch 29/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.7098 - val_loss: 64.7627\n",
      "Epoch 30/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.6296 - val_loss: 64.7485\n",
      "Epoch 31/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.5608 - val_loss: 64.6470\n",
      "Epoch 32/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.5015 - val_loss: 64.5971\n",
      "Epoch 33/150\n",
      "240/787 [========>.....................] - ETA: 11s - loss: 63.8289\n",
      "Epoch 33: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0033.ckpt\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.4563 - val_loss: 64.5614\n",
      "Epoch 34/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.4048 - val_loss: 64.5351\n",
      "Epoch 35/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.3669 - val_loss: 64.4726\n",
      "Epoch 36/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.3301 - val_loss: 64.4514\n",
      "Epoch 37/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.2905 - val_loss: 64.4208\n",
      "Epoch 38/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.2635 - val_loss: 64.4100\n",
      "Epoch 39/150\n",
      "603/787 [=====================>........] - ETA: 3s - loss: 63.2661\n",
      "Epoch 39: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0039.ckpt\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.2341 - val_loss: 64.3887\n",
      "Epoch 40/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.2085 - val_loss: 64.3702\n",
      "Epoch 41/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.1863 - val_loss: 64.3623\n",
      "Epoch 42/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.1746 - val_loss: 64.3673\n",
      "Epoch 43/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.1584 - val_loss: 64.3474\n",
      "Epoch 44/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.1448 - val_loss: 64.3469\n",
      "Epoch 45/150\n",
      "787/787 [==============================] - 18s 23ms/step - loss: 63.1379 - val_loss: 64.3525\n",
      "Epoch 46/150\n",
      "178/787 [=====>........................] - ETA: 12s - loss: 63.4987\n",
      "Epoch 46: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0046.ckpt\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.1253 - val_loss: 64.3507\n",
      "Epoch 47/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.1157 - val_loss: 64.3417\n",
      "Epoch 48/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.1059 - val_loss: 64.3347\n",
      "Epoch 49/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0922 - val_loss: 64.3254\n",
      "Epoch 50/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0844 - val_loss: 64.3249\n",
      "Epoch 51/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0808 - val_loss: 64.3117\n",
      "Epoch 52/150\n",
      "540/787 [===================>..........] - ETA: 5s - loss: 63.1296\n",
      "Epoch 52: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0052.ckpt\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0706 - val_loss: 64.3077\n",
      "Epoch 53/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0674 - val_loss: 64.3097\n",
      "Epoch 54/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0618 - val_loss: 64.3034\n",
      "Epoch 55/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0560 - val_loss: 64.2929\n",
      "Epoch 56/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0510 - val_loss: 64.2917\n",
      "Epoch 57/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0505 - val_loss: 64.2936\n",
      "Epoch 58/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0468 - val_loss: 64.2923\n",
      "Epoch 59/150\n",
      "118/787 [===>..........................] - ETA: 13s - loss: 63.4930\n",
      "Epoch 59: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0059.ckpt\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0439 - val_loss: 64.2892\n",
      "Epoch 60/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0421 - val_loss: 64.2848\n",
      "Epoch 61/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0391 - val_loss: 64.2853\n",
      "Epoch 62/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0364 - val_loss: 64.2797\n",
      "Epoch 63/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0347 - val_loss: 64.2798\n",
      "Epoch 64/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0332 - val_loss: 64.2727\n",
      "Epoch 65/150\n",
      "479/787 [=================>............] - ETA: 6s - loss: 63.1692\n",
      "Epoch 65: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0065.ckpt\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0310 - val_loss: 64.2698\n",
      "Epoch 66/150\n",
      "787/787 [==============================] - 18s 23ms/step - loss: 63.0310 - val_loss: 64.2669\n",
      "Epoch 67/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0308 - val_loss: 64.2683\n",
      "Epoch 68/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0295 - val_loss: 64.2644\n",
      "Epoch 69/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0278 - val_loss: 64.2618\n",
      "Epoch 70/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0271 - val_loss: 64.2614\n",
      "Epoch 71/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0262 - val_loss: 64.2618\n",
      "Epoch 72/150\n",
      " 55/787 [=>............................] - ETA: 15s - loss: 63.4286\n",
      "Epoch 72: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0072.ckpt\n",
      "787/787 [==============================] - 18s 23ms/step - loss: 63.0251 - val_loss: 64.2553\n",
      "Epoch 73/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0244 - val_loss: 64.2575\n",
      "Epoch 74/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0246 - val_loss: 64.2529\n",
      "Epoch 75/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0236 - val_loss: 64.2508\n",
      "Epoch 76/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0227 - val_loss: 64.2494\n",
      "Epoch 77/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0223 - val_loss: 64.2486\n",
      "Epoch 78/150\n",
      "420/787 [===============>..............] - ETA: 7s - loss: 63.2105\n",
      "Epoch 78: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0078.ckpt\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0219 - val_loss: 64.2467\n",
      "Epoch 79/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0212 - val_loss: 64.2457\n",
      "Epoch 80/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0208 - val_loss: 64.2439\n",
      "Epoch 81/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0203 - val_loss: 64.2434\n",
      "Epoch 82/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0200 - val_loss: 64.2421\n",
      "Epoch 83/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0195 - val_loss: 64.2406\n",
      "Epoch 84/150\n",
      "781/787 [============================>.] - ETA: 0s - loss: 63.0291\n",
      "Epoch 84: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0084.ckpt\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0194 - val_loss: 64.2402\n",
      "Epoch 85/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0191 - val_loss: 64.2396\n",
      "Epoch 86/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0188 - val_loss: 64.2382\n",
      "Epoch 87/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0186 - val_loss: 64.2375\n",
      "Epoch 88/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0184 - val_loss: 64.2376\n",
      "Epoch 89/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0183 - val_loss: 64.2362\n",
      "Epoch 90/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0180 - val_loss: 64.2359\n",
      "Epoch 91/150\n",
      "359/787 [============>.................] - ETA: 9s - loss: 63.2616\n",
      "Epoch 91: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0091.ckpt\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0178 - val_loss: 64.2356\n",
      "Epoch 92/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0178 - val_loss: 64.2348\n",
      "Epoch 93/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0176 - val_loss: 64.2347\n",
      "Epoch 94/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0175 - val_loss: 64.2344\n",
      "Epoch 95/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0174 - val_loss: 64.2340\n",
      "Epoch 96/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0173 - val_loss: 64.2337\n",
      "Epoch 97/150\n",
      "722/787 [==========================>...] - ETA: 1s - loss: 63.0292\n",
      "Epoch 97: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0097.ckpt\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0172 - val_loss: 64.2335\n",
      "Epoch 98/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0171 - val_loss: 64.2333\n",
      "Epoch 99/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0170 - val_loss: 64.2331\n",
      "Epoch 100/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0169 - val_loss: 64.2328\n",
      "Epoch 101/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0168 - val_loss: 64.2326\n",
      "Epoch 102/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0168 - val_loss: 64.2325\n",
      "Epoch 103/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0167 - val_loss: 64.2322\n",
      "Epoch 104/150\n",
      "297/787 [==========>...................] - ETA: 10s - loss: 63.3296\n",
      "Epoch 104: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0104.ckpt\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0166 - val_loss: 64.2322\n",
      "Epoch 105/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0166 - val_loss: 64.2319\n",
      "Epoch 106/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0165 - val_loss: 64.2319\n",
      "Epoch 107/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0165 - val_loss: 64.2317\n",
      "Epoch 108/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0165 - val_loss: 64.2316\n",
      "Epoch 109/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0164 - val_loss: 64.2315\n",
      "Epoch 110/150\n",
      "659/787 [========================>.....] - ETA: 2s - loss: 63.0409\n",
      "Epoch 110: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0110.ckpt\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0164 - val_loss: 64.2314\n",
      "Epoch 111/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0163 - val_loss: 64.2313\n",
      "Epoch 112/150\n",
      "787/787 [==============================] - 18s 23ms/step - loss: 63.0163 - val_loss: 64.2313\n",
      "Epoch 113/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0162 - val_loss: 64.2312\n",
      "Epoch 114/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0162 - val_loss: 64.2312\n",
      "Epoch 115/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0162 - val_loss: 64.2311\n",
      "Epoch 116/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0162 - val_loss: 64.2311\n",
      "Epoch 117/150\n",
      "235/787 [=======>......................] - ETA: 11s - loss: 63.3654\n",
      "Epoch 117: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0117.ckpt\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0162 - val_loss: 64.2310\n",
      "Epoch 118/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0162 - val_loss: 64.2310\n",
      "Epoch 119/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0161 - val_loss: 64.2310\n",
      "Epoch 120/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0162 - val_loss: 64.2310\n",
      "Epoch 121/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0161 - val_loss: 64.2310\n",
      "Epoch 122/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0161 - val_loss: 64.2310\n",
      "Epoch 123/150\n",
      "599/787 [=====================>........] - ETA: 4s - loss: 63.0353\n",
      "Epoch 123: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0123.ckpt\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0161 - val_loss: 64.2309\n",
      "Epoch 124/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0161 - val_loss: 64.2309\n",
      "Epoch 125/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0161 - val_loss: 64.2309\n",
      "Epoch 126/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0161 - val_loss: 64.2309\n",
      "Epoch 127/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0161 - val_loss: 64.2309\n",
      "Epoch 128/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0161 - val_loss: 64.2309\n",
      "Epoch 129/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0161 - val_loss: 64.2309\n",
      "Epoch 130/150\n",
      "175/787 [=====>........................] - ETA: 12s - loss: 63.4394\n",
      "Epoch 130: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0130.ckpt\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0161 - val_loss: 64.2309\n",
      "Epoch 131/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0160 - val_loss: 64.2309\n",
      "Epoch 132/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0160 - val_loss: 64.2309\n",
      "Epoch 133/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0160 - val_loss: 64.2309\n",
      "Epoch 134/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0160 - val_loss: 64.2309\n",
      "Epoch 135/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0160 - val_loss: 64.2309\n",
      "Epoch 136/150\n",
      "539/787 [===================>..........] - ETA: 5s - loss: 63.0757\n",
      "Epoch 136: saving model to training/2023_Apr_20_07_20_12_classic_128_64_32_1/cp-0136.ckpt\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0160 - val_loss: 64.2309\n",
      "Epoch 137/150\n",
      "787/787 [==============================] - 19s 23ms/step - loss: 63.0160 - val_loss: 64.2309\n",
      "Epoch 138/150\n",
      "787/787 [==============================] - 17s 22ms/step - loss: 63.0160 - val_loss: 64.2309\n",
      "Epoch 139/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0160 - val_loss: 64.2309\n",
      "Epoch 140/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0160 - val_loss: 64.2309\n",
      "Epoch 141/150\n",
      "787/787 [==============================] - 17s 21ms/step - loss: 63.0160 - val_loss: 64.2309\n",
      "Epoch 142/150\n",
      "787/787 [==============================] - 18s 22ms/step - loss: 63.0160 - val_loss: 64.2309\n",
      "Epoch 142: early stopping\n",
      "INFO:tensorflow:Assets written to: training/2023_Apr_20_07_20_12_classic_128_64_32_1/assets\n",
      "Training end. Current Time = 2023_Apr_20_08_01_14\n",
      "CPU times: user 2h 19min 9s, sys: 33min 38s, total: 2h 52min 47s\n",
      "Wall time: 41min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "current_time = datetime.now().strftime(\"%Y_%b_%d_%H_%M_%S\")\n",
    "print(\"Training start. Current Time =\", current_time)\n",
    "\n",
    "nEpochs = 150\n",
    "\n",
    "log_dir = \"logs/fit/\" + current_time + dir_postfix\n",
    "job_dir = \"training/\" + current_time + dir_postfix\n",
    "\n",
    "checkpoint_path = job_dir + \"/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1,\n",
    "                                                 save_freq = 5085)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=(10, 20))\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=5, verbose=1)\n",
    "callbacks =  [tensorboard_callback, cp_callback, early_stop_callback]\n",
    "\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "   \n",
    "model.fit(dataset.skip(10), \n",
    "          epochs=nEpochs, shuffle=True,\n",
    "          callbacks=callbacks,\n",
    "          validation_data = dataset.take(10)\n",
    "            )\n",
    "model.save(job_dir, save_format='tf')\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y_%b_%d_%H_%M_%S\")\n",
    "print(\"Training end. Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b3f82-bcf2-40f4-8811-b63924cf8f59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
