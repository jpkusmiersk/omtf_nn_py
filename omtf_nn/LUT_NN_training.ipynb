{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e108fe-72a6-4053-bec2-af1a368594f2",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d419c1d5-26ec-42d6-a6fb-e076dd903666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, time\n",
    "from datetime import datetime\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e9384f-3b1f-4210-b8cf-9cda27c00a8a",
   "metadata": {},
   "source": [
    "## Networks definitions and adaptations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef53a8d-4d7c-4d1d-b914-8429d43fa8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUT NN definitions:\n",
      "layer1_lut_size 1024\n",
      "layer2_lut_size 256\n",
      "layer3_lut_size 32\n",
      "layer2_lutRangesCnt 16\n",
      "layer2_input_offset 8.0\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "from architecture_definitions import *\n",
    "\n",
    "oneOverPt = False \n",
    "lut_nn = True\n",
    "output_type = 0\n",
    "last_input_is_bias = True\n",
    "\n",
    "if output_type == 1:\n",
    "    layer3_neurons = 3\n",
    "    loss_fn = custom_loss3\n",
    "else: \n",
    "    output_cnt = 1\n",
    "    layer3_neurons = 1\n",
    "    loss_fn = 'mape'\n",
    "        \n",
    "if not last_input_is_bias:\n",
    "    networkInputSize =  nLayers\n",
    "    layer2_lutRangesCnt = 1\n",
    "    layer2_input_offset = None \n",
    "    \n",
    "    \n",
    " \n",
    "dir_postfix = get_lut_nn_dir_postfix() \n",
    "    \n",
    "print_LUT_NN()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630c86e0-bf1f-4a56-abf8-5762ea739a8f",
   "metadata": {},
   "source": [
    "### Training data set preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "165627dd-9c1c-41ea-b97a-ff97949c379a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from files:\n",
      "/scratch_cmsse/alibordi/data/training/SingleMu_OneOverPt_April4_chunk_0_filtered.tfrecord.gzip\n",
      "/scratch_cmsse/alibordi/data/training/SingleMu_OneOverPt_Feb15_chunk_0_filtered.tfrecord.gzip\n",
      "/scratch_cmsse/alibordi/data/training/SingleMu_OneOverPt_Feb22_chunk_0_filtered.tfrecord.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 17:39:35.016656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-19 17:39:35.020284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-19 17:39:35.020395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-19 17:39:35.020756: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-19 17:39:35.021678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-19 17:39:35.021809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-19 17:39:35.021895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-19 17:39:35.423218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-19 17:39:35.423343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-19 17:39:35.423443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-19 17:39:35.423523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6658 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import io_functions as io\n",
    "importlib.reload(io)\n",
    "\n",
    "batchSize = 4096\n",
    "nEpochs = 1\n",
    "\n",
    "#trainDataDir = \"/scratch_ssd/akalinow/ProgrammingProjects/MachineLearning/OMTF/data/18_12_2020/\"   \n",
    "trainDataDir = \"/home/kbunkow/cms_data/OMTF_data_2020/18_12_2020/\"\n",
    "trainFileNames = glob.glob(trainDataDir+'OMTFHits_pats0x0003_oldSample_files_*_chunk_0.tfrecord.gzip')\n",
    "\n",
    "trainDataDir = \"/scratch_cmsse/alibordi/data/training/\"\n",
    "#trainDataDir = \"/home/kbunkow/cms_data/SingleMu/TFRecord/training/\"\n",
    "#trainDataDir = \"/eos/user/a/akalinow/Data/SingleMu/TFRecord/training/\"\n",
    "trainFileNames = glob.glob(trainDataDir+'*OneOverPt*tfrecord.gzip')\n",
    "\n",
    "dataset = io.get_LUT_NN_dataset(batchSize, nEpochs, trainFileNames, \n",
    "                                nRefLayers=nRefLayers,\n",
    "                                layer1_lut_size=layer1_lut_size,\n",
    "                                layer2_lut_size=layer2_lut_size,\n",
    "                                layer2_lutRangesCnt=layer2_lutRangesCnt,\n",
    "                                last_input_is_bias=last_input_is_bias,\n",
    "                                rangeFactor=rangeFactor,\n",
    "                                isTrain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44abffd6-e9d9-47f0-9198-a84e8c7ac6b6",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d414507a-ce56-42f5-8edf-0a4064d77c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing LutInterLayer  layer1 lut_size 1024 num_inputs 19 num_outputs 16 input_offset 0 self.input_offset 0 last_input_is_bias True\n",
      "write_lut_hist False hist_writer None\n",
      "constructing LutInterLayer  layer2 lut_size 256 num_inputs 16 num_outputs 8 input_offset 8.0 self.input_offset 8.0 last_input_is_bias False\n",
      "write_lut_hist False hist_writer None\n",
      "constructing LutInterLayer  layer3 lut_size 32 num_inputs 8 num_outputs 1 input_offset None self.input_offset 15.5 last_input_is_bias False\n",
      "write_lut_hist False hist_writer None\n",
      "layer1 \n",
      "LutInterLayer.build: luts_float: layer1.luts_float:0 shape (18, 1024, 16)\n",
      "layer2 \n",
      "LutInterLayer.build: luts_float: layer2.luts_float:0 shape (16, 256, 8)\n",
      "layer3 \n",
      "LutInterLayer.build: luts_float: layer3.luts_float:0 shape (8, 32, 1)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (LutInterLayer)      (None, 16)                313344    \n",
      "                                                                 \n",
      " layer2 (LutInterLayer)      (None, 8)                 36864     \n",
      "                                                                 \n",
      " layer3 (LutInterLayer)      (None, 1)                 512       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 350,720\n",
      "Trainable params: 327,936\n",
      "Non-trainable params: 22,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import model_functions as models\n",
    "importlib.reload(models)\n",
    "\n",
    "loss_fn = loss_MAPE_MAE\n",
    "\n",
    "model = models.get_LUT_NN(last_input_is_bias=last_input_is_bias, loss_fn=loss_fn)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42a01e5-c7aa-4f68-87f1-c6a4e6270723",
   "metadata": {},
   "source": [
    "### The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a48d01be-95ac-416b-8577-c23d3f8924b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start. Current Time = 2023_Apr_19_17_39_36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 17:39:36.540746: I tensorflow/core/profiler/lib/profiler_session.cc:99] Profiler session initializing.\n",
      "2023-04-19 17:39:36.540788: I tensorflow/core/profiler/lib/profiler_session.cc:114] Profiler session started.\n",
      "2023-04-19 17:39:36.540824: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1665] Profiler found 1 GPUs\n",
      "2023-04-19 17:39:36.671731: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session tear down.\n",
      "2023-04-19 17:39:36.673103: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1799] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "     14/Unknown - 2s 23ms/step - loss: 337.9818"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 17:39:38.222835: I tensorflow/core/profiler/lib/profiler_session.cc:99] Profiler session initializing.\n",
      "2023-04-19 17:39:38.222860: I tensorflow/core/profiler/lib/profiler_session.cc:114] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18/Unknown - 2s 25ms/step - loss: 328.5387"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 17:39:38.562408: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2023-04-19 17:39:38.572197: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1799] CUPTI activity buffer flushed\n",
      "2023-04-19 17:39:38.899702: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:521]  GpuTracer has collected 1803 callback api events and 1782 activity events. \n",
      "2023-04-19 17:39:39.112261: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session tear down.\n",
      "2023-04-19 17:39:39.727211: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/fit/2023_Apr_19_17_39_36_lut_16_8_1/plugins/profile/2023_04_19_17_39_39\n",
      "\n",
      "2023-04-19 17:39:40.399392: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to logs/fit/2023_Apr_19_17_39_36_lut_16_8_1/plugins/profile/2023_04_19_17_39_39/fba800be2533.trace.json.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     30/Unknown - 4s 95ms/step - loss: 310.7827 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 17:39:40.650579: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/fit/2023_Apr_19_17_39_36_lut_16_8_1/plugins/profile/2023_04_19_17_39_39\n",
      "\n",
      "2023-04-19 17:39:40.656763: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/2023_Apr_19_17_39_36_lut_16_8_1/plugins/profile/2023_04_19_17_39_39/fba800be2533.memory_profile.json.gz\n",
      "2023-04-19 17:39:40.662850: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/fit/2023_Apr_19_17_39_36_lut_16_8_1/plugins/profile/2023_04_19_17_39_39\n",
      "Dumped tool data for xplane.pb to logs/fit/2023_Apr_19_17_39_36_lut_16_8_1/plugins/profile/2023_04_19_17_39_39/fba800be2533.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/fit/2023_Apr_19_17_39_36_lut_16_8_1/plugins/profile/2023_04_19_17_39_39/fba800be2533.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/fit/2023_Apr_19_17_39_36_lut_16_8_1/plugins/profile/2023_04_19_17_39_39/fba800be2533.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/fit/2023_Apr_19_17_39_36_lut_16_8_1/plugins/profile/2023_04_19_17_39_39/fba800be2533.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/fit/2023_Apr_19_17_39_36_lut_16_8_1/plugins/profile/2023_04_19_17_39_39/fba800be2533.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 [==============================] - 9s 33ms/step - loss: 186.6287 - val_loss: 106.4684\n",
      "Epoch 2/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 85.2020 - val_loss: 72.8849\n",
      "Epoch 3/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 71.3776 - val_loss: 71.8170\n",
      "Epoch 4/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 68.5587 - val_loss: 70.5415\n",
      "Epoch 5/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 67.1098 - val_loss: 68.2853\n",
      "Epoch 6/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 66.1344 - val_loss: 66.7443\n",
      "Epoch 7/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 65.5026 - val_loss: 66.6992\n",
      "Epoch 8/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 64.8641 - val_loss: 66.0042\n",
      "Epoch 9/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 64.3853 - val_loss: 65.7552\n",
      "Epoch 10/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 64.0272 - val_loss: 66.1117\n",
      "Epoch 11/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 63.6894 - val_loss: 66.7280\n",
      "Epoch 12/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 63.3795 - val_loss: 66.2219\n",
      "Epoch 13/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 63.1223 - val_loss: 65.8221\n",
      "Epoch 14/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 62.9169 - val_loss: 65.3068\n",
      "Epoch 15/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 62.6515 - val_loss: 65.8915\n",
      "Epoch 16/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 62.4827 - val_loss: 65.5359\n",
      "Epoch 17/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 62.2832 - val_loss: 65.2393\n",
      "Epoch 18/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 62.0916 - val_loss: 65.2292\n",
      "Epoch 19/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 61.9358 - val_loss: 65.1098\n",
      "Epoch 20/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 61.7798 - val_loss: 65.3213\n",
      "Epoch 21/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 61.6060 - val_loss: 65.1928\n",
      "Epoch 22/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 61.4759 - val_loss: 65.4834\n",
      "Epoch 23/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 61.3144 - val_loss: 65.3400\n",
      "Epoch 24/150\n",
      "  1/221 [..............................] - ETA: 50s - loss: 60.9285\n",
      "Epoch 24: saving model to training/2023_Apr_19_17_39_36_lut_16_8_1/cp-0024.ckpt\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 61.2068 - val_loss: 65.1874\n",
      "Epoch 25/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 61.0844 - val_loss: 65.1468\n",
      "Epoch 26/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 60.9734 - val_loss: 65.2690\n",
      "Epoch 27/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 60.8643 - val_loss: 65.2887\n",
      "Epoch 28/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 60.7690 - val_loss: 65.3469\n",
      "Epoch 29/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 60.6647 - val_loss: 65.2393\n",
      "Epoch 30/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 60.5481 - val_loss: 65.3504\n",
      "Epoch 31/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 60.4529 - val_loss: 65.1867\n",
      "Epoch 32/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 60.3533 - val_loss: 65.2259\n",
      "Epoch 33/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 60.2886 - val_loss: 65.3706\n",
      "Epoch 34/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 60.1718 - val_loss: 65.3066\n",
      "Epoch 35/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 60.1113 - val_loss: 65.4440\n",
      "Epoch 36/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 60.0181 - val_loss: 65.2169\n",
      "Epoch 37/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 59.9550 - val_loss: 65.4320\n",
      "Epoch 38/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 59.8638 - val_loss: 65.4407\n",
      "Epoch 39/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 59.7977 - val_loss: 65.3403\n",
      "Epoch 40/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 59.7378 - val_loss: 65.4389\n",
      "Epoch 41/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 59.6513 - val_loss: 65.5137\n",
      "Epoch 42/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 59.6155 - val_loss: 65.5824\n",
      "Epoch 43/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 59.5398 - val_loss: 65.5366\n",
      "Epoch 44/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 59.4953 - val_loss: 65.6014\n",
      "Epoch 45/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 59.4163 - val_loss: 65.5890\n",
      "Epoch 46/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 59.3623 - val_loss: 65.6594\n",
      "Epoch 47/150\n",
      "  1/221 [..............................] - ETA: 51s - loss: 58.2470\n",
      "Epoch 47: saving model to training/2023_Apr_19_17_39_36_lut_16_8_1/cp-0047.ckpt\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 59.3218 - val_loss: 65.6241\n",
      "Epoch 48/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 59.2596 - val_loss: 65.7098\n",
      "Epoch 49/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 59.2129 - val_loss: 65.7721\n",
      "Epoch 50/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 59.1561 - val_loss: 65.6160\n",
      "Epoch 51/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 59.1157 - val_loss: 65.5729\n",
      "Epoch 52/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 59.0594 - val_loss: 65.5666\n",
      "Epoch 53/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 59.0128 - val_loss: 65.6676\n",
      "Epoch 54/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 58.9661 - val_loss: 65.6056\n",
      "Epoch 55/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 58.9295 - val_loss: 65.7715\n",
      "Epoch 56/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 58.8870 - val_loss: 65.6611\n",
      "Epoch 57/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 58.8402 - val_loss: 65.8232\n",
      "Epoch 58/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 58.8043 - val_loss: 65.8357\n",
      "Epoch 59/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 58.7606 - val_loss: 65.7358\n",
      "Epoch 60/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 58.7149 - val_loss: 65.8139\n",
      "Epoch 61/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 58.6989 - val_loss: 65.8701\n",
      "Epoch 62/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 58.6485 - val_loss: 65.8997\n",
      "Epoch 63/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 58.6085 - val_loss: 65.8158\n",
      "Epoch 64/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 58.5785 - val_loss: 65.8247\n",
      "Epoch 65/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 58.5581 - val_loss: 65.8598\n",
      "Epoch 66/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 58.5157 - val_loss: 65.8549\n",
      "Epoch 67/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 58.4858 - val_loss: 65.8623\n",
      "Epoch 68/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 58.4616 - val_loss: 65.8775\n",
      "Epoch 69/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 58.4260 - val_loss: 65.9376\n",
      "Epoch 70/150\n",
      "  4/221 [..............................] - ETA: 4s - loss: 57.9885 \n",
      "Epoch 70: saving model to training/2023_Apr_19_17_39_36_lut_16_8_1/cp-0070.ckpt\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 58.3874 - val_loss: 65.8663\n",
      "Epoch 71/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 58.3635 - val_loss: 65.9328\n",
      "Epoch 72/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 58.3357 - val_loss: 65.9347\n",
      "Epoch 73/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 58.3119 - val_loss: 65.9545\n",
      "Epoch 74/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 58.2847 - val_loss: 66.0082\n",
      "Epoch 75/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 58.2596 - val_loss: 66.0523\n",
      "Epoch 76/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 58.2293 - val_loss: 66.0204\n",
      "Epoch 77/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 58.2100 - val_loss: 66.0408\n",
      "Epoch 78/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 58.1845 - val_loss: 66.0303\n",
      "Epoch 79/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 58.1571 - val_loss: 66.0812\n",
      "Epoch 80/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 58.1292 - val_loss: 66.0571\n",
      "Epoch 81/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 58.1119 - val_loss: 66.0713\n",
      "Epoch 82/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 58.0899 - val_loss: 66.0733\n",
      "Epoch 83/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 58.0674 - val_loss: 66.0601\n",
      "Epoch 84/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 58.0439 - val_loss: 66.0429\n",
      "Epoch 85/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 58.0234 - val_loss: 66.0234\n",
      "Epoch 86/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 58.0067 - val_loss: 66.0847\n",
      "Epoch 87/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.9889 - val_loss: 66.0630\n",
      "Epoch 88/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.9758 - val_loss: 66.0784\n",
      "Epoch 89/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.9556 - val_loss: 66.0874\n",
      "Epoch 90/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.9413 - val_loss: 66.0993\n",
      "Epoch 91/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.9227 - val_loss: 66.1097\n",
      "Epoch 92/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.9090 - val_loss: 66.0978\n",
      "Epoch 93/150\n",
      "  7/221 [..............................] - ETA: 4s - loss: 57.9732\n",
      "Epoch 93: saving model to training/2023_Apr_19_17_39_36_lut_16_8_1/cp-0093.ckpt\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.8919 - val_loss: 66.0994\n",
      "Epoch 94/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.8726 - val_loss: 66.1265\n",
      "Epoch 95/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.8557 - val_loss: 66.1603\n",
      "Epoch 96/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.8513 - val_loss: 66.1322\n",
      "Epoch 97/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.8339 - val_loss: 66.1601\n",
      "Epoch 98/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.8138 - val_loss: 66.1665\n",
      "Epoch 99/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.8036 - val_loss: 66.1761\n",
      "Epoch 100/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.7927 - val_loss: 66.2045\n",
      "Epoch 101/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.7776 - val_loss: 66.1764\n",
      "Epoch 102/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.7675 - val_loss: 66.2149\n",
      "Epoch 103/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.7557 - val_loss: 66.2064\n",
      "Epoch 104/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.7435 - val_loss: 66.1883\n",
      "Epoch 105/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.7326 - val_loss: 66.1948\n",
      "Epoch 106/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.7216 - val_loss: 66.1898\n",
      "Epoch 107/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.7127 - val_loss: 66.1917\n",
      "Epoch 108/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.7043 - val_loss: 66.1940\n",
      "Epoch 109/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.6929 - val_loss: 66.1999\n",
      "Epoch 110/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.6866 - val_loss: 66.1954\n",
      "Epoch 111/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.6751 - val_loss: 66.2043\n",
      "Epoch 112/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.6663 - val_loss: 66.1901\n",
      "Epoch 113/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.6588 - val_loss: 66.2041\n",
      "Epoch 114/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.6491 - val_loss: 66.2164\n",
      "Epoch 115/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.6480 - val_loss: 66.2157\n",
      "Epoch 116/150\n",
      "  8/221 [>.............................] - ETA: 5s - loss: 57.5756\n",
      "Epoch 116: saving model to training/2023_Apr_19_17_39_36_lut_16_8_1/cp-0116.ckpt\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.6371 - val_loss: 66.2134\n",
      "Epoch 117/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.6304 - val_loss: 66.2104\n",
      "Epoch 118/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.6199 - val_loss: 66.2131\n",
      "Epoch 119/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.6159 - val_loss: 66.2194\n",
      "Epoch 120/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.6118 - val_loss: 66.2278\n",
      "Epoch 121/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.6066 - val_loss: 66.2318\n",
      "Epoch 122/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.5998 - val_loss: 66.2274\n",
      "Epoch 123/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.5932 - val_loss: 66.2367\n",
      "Epoch 124/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.5900 - val_loss: 66.2311\n",
      "Epoch 125/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.5839 - val_loss: 66.2503\n",
      "Epoch 126/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.5802 - val_loss: 66.2403\n",
      "Epoch 127/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.5762 - val_loss: 66.2422\n",
      "Epoch 128/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.5741 - val_loss: 66.2342\n",
      "Epoch 129/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.5673 - val_loss: 66.2405\n",
      "Epoch 130/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.5678 - val_loss: 66.2432\n",
      "Epoch 131/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.5599 - val_loss: 66.2382\n",
      "Epoch 132/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.5576 - val_loss: 66.2372\n",
      "Epoch 133/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.5545 - val_loss: 66.2361\n",
      "Epoch 134/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.5537 - val_loss: 66.2295\n",
      "Epoch 135/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.5502 - val_loss: 66.2304\n",
      "Epoch 136/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.5481 - val_loss: 66.2305\n",
      "Epoch 137/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.5461 - val_loss: 66.2211\n",
      "Epoch 138/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.5411 - val_loss: 66.2223\n",
      "Epoch 139/150\n",
      " 10/221 [>.............................] - ETA: 4s - loss: 57.3272\n",
      "Epoch 139: saving model to training/2023_Apr_19_17_39_36_lut_16_8_1/cp-0139.ckpt\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.5383 - val_loss: 66.2222\n",
      "Epoch 140/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.5357 - val_loss: 66.2215\n",
      "Epoch 141/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.5308 - val_loss: 66.2178\n",
      "Epoch 142/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.5264 - val_loss: 66.2190\n",
      "Epoch 143/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.5225 - val_loss: 66.2208\n",
      "Epoch 144/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.5215 - val_loss: 66.2217\n",
      "Epoch 145/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.5200 - val_loss: 66.2211\n",
      "Epoch 146/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.5181 - val_loss: 66.2177\n",
      "Epoch 147/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.5160 - val_loss: 66.2225\n",
      "Epoch 148/150\n",
      "221/221 [==============================] - 5s 23ms/step - loss: 57.5126 - val_loss: 66.2214\n",
      "Epoch 149/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.5130 - val_loss: 66.2280\n",
      "Epoch 150/150\n",
      "221/221 [==============================] - 5s 22ms/step - loss: 57.5079 - val_loss: 66.2265\n",
      "INFO:tensorflow:Assets written to: training/2023_Apr_19_17_39_36_lut_16_8_1/assets\n",
      "Training end. Current Time = 2023_Apr_19_17_52_39\n",
      "CPU times: user 45min 12s, sys: 10min 43s, total: 55min 55s\n",
      "Wall time: 13min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "current_time = datetime.now().strftime(\"%Y_%b_%d_%H_%M_%S\")\n",
    "print(\"Training start. Current Time =\", current_time)\n",
    "\n",
    "nEpochs = 150\n",
    "\n",
    "log_dir = \"logs/fit/\" + current_time + dir_postfix\n",
    "job_dir = \"training/\" + current_time + dir_postfix\n",
    "\n",
    "checkpoint_path = job_dir + \"/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1,\n",
    "                                                 save_freq = 5085)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=(10, 20))\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=5, verbose=1)\n",
    "callbacks =  [tensorboard_callback, cp_callback, early_stop_callback]\n",
    "\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "   \n",
    "model.fit(dataset.skip(10), \n",
    "          epochs=nEpochs, shuffle=True,\n",
    "          callbacks=[tensorboard_callback, cp_callback],\n",
    "          validation_data = dataset.take(10)\n",
    "            )\n",
    "model.save(job_dir, save_format='tf')\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y_%b_%d_%H_%M_%S\")\n",
    "print(\"Training end. Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7df34b-7296-4e0f-878c-cffe6e277b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
